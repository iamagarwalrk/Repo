{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469b81ed",
   "metadata": {},
   "source": [
    "# Understanding Gradient Descent for Linear Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this Jupyter Notebook, where we delve into the fundamental concept of Gradient Descent in the context of Linear Regression. In this project, our goal is to demystify the mechanics of gradient descent and its application to find optimal parameters for a simple linear regression model.\n",
    "\n",
    "### The Project Overview\n",
    "\n",
    "#### Step 1: Initializing Dummy Values\n",
    "We kick off our exploration by initializing dummy values for the independent variable `x` and the dependent variable `y`. This step lays the foundation for our linear regression model, creating a synthetic dataset for experimentation.\n",
    "\n",
    "#### Step 2: Initializing Model Parameters\n",
    "Next, we set the initial values for the model parameters `b0` and `b1` in the equation `yhat = b0 + b1*x`. These parameters represent the intercept and slope of our linear regression line.\n",
    "\n",
    "#### Step 3: Defining Learning Rate\n",
    "The learning rate is a critical hyperparameter in the gradient descent process. We define its value to control the step size during the optimization, influencing the convergence speed and stability of the algorithm.\n",
    "\n",
    "#### Step 4: Defining the Gradient Descent Function\n",
    "The heart of our project lies in the implementation of the gradient descent algorithm. We define a function that iteratively updates the parameters `b0` and `b1` based on the calculated gradients, moving us closer to the optimal values that minimize the error in our linear regression model.\n",
    "\n",
    "#### Step 5: Calling the Gradient Descent Function\n",
    "With all components in place, we call our gradient descent function for a defined number of epochs. Each epoch represents a complete pass through the dataset, allowing the algorithm to refine the parameter estimates and converge towards the optimal values.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Throughout this notebook, we will:\n",
    "\n",
    "- Gain insights into the inner workings of gradient descent for linear regression.\n",
    "- Visualize the progression of parameter updates over epochs.\n",
    "- Understand the impact of the learning rate on convergence.\n",
    "- Explore how gradient descent iteratively refines the model for better predictions.\n",
    "\n",
    "So, join us on this journey to grasp the essentials of gradient descent in the context of linear regression and witness firsthand how it optimizes our model for more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1f17d",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbba3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65f73a",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ea2a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82520658]\n",
      " [-0.00619132]\n",
      " [-1.21642199]\n",
      " [-0.4967771 ]\n",
      " [ 0.03278211]\n",
      " [ 0.75782988]\n",
      " [ 0.40746266]\n",
      " [-1.39257024]\n",
      " [-0.22905686]\n",
      " [-0.66839719]\n",
      " [-0.65254061]\n",
      " [-0.58985634]\n",
      " [-1.35823861]\n",
      " [-0.67070794]\n",
      " [-0.45241266]\n",
      " [-0.69356342]\n",
      " [ 0.79972314]\n",
      " [ 1.14203011]\n",
      " [ 0.63252591]\n",
      " [ 1.26975853]] [[ 2.39831074]\n",
      " [ 0.73551494]\n",
      " [-1.68494639]\n",
      " [-0.24565661]\n",
      " [ 0.81346181]\n",
      " [ 2.26355735]\n",
      " [ 1.5628229 ]\n",
      " [-2.03724289]\n",
      " [ 0.28978387]\n",
      " [-0.58889678]\n",
      " [-0.55718363]\n",
      " [-0.4318151 ]\n",
      " [-1.96857964]\n",
      " [-0.59351829]\n",
      " [-0.15692773]\n",
      " [-0.63922925]\n",
      " [ 2.34734386]\n",
      " [ 3.03195782]\n",
      " [ 2.01294941]\n",
      " [ 3.28741464]]\n"
     ]
    }
   ],
   "source": [
    "x=np.random.randn(20, 1)\n",
    "y=2*x + np.random.rand()\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7533488",
   "metadata": {},
   "source": [
    "### Initialize the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95ba92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze value of b0, b1 for (yhat=b0 + b1*x)\n",
    "\n",
    "b0, b1=0.0, 0.1\n",
    "\n",
    "# Define Learning rate\n",
    "\n",
    "learning_rate=0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545dde1",
   "metadata": {},
   "source": [
    "## Define Gradient Descent Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ec5b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, b0, b1, learning_rate):\n",
    "    \n",
    "    # Cost function= (y-(b0+b1*x))**2/2*N and b(new)=b(old)-L(dCf/db)\n",
    "    N=x.shape[0]\n",
    "    dCFdb0, dCFdb1=0.0, 0.0\n",
    "    \n",
    "    for xi, yi in zip(x, y):\n",
    "        dCFdb0-=(yi-(b0+b1*xi))\n",
    "        dCFdb1-=b1*(yi-(b0+b1*xi))\n",
    "        \n",
    "    b0=b0-learning_rate*(dCFdb0)/N\n",
    "    b1=b1-learning_rate*(dCFdb1)/N\n",
    "    \n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call the function for defined epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8478cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss is [2.29022716], parameter b0:[0.3240755] and b1:[0.13487426]\n",
      "1 loss is [2.24447768], parameter b0:[0.39813171] and b1:[0.14486254]\n",
      "2 loss is [2.22087204], parameter b0:[0.44307671] and b1:[0.15137339]\n",
      "3 loss is [2.20803696], parameter b0:[0.470377] and b1:[0.15550592]\n",
      "4 loss is [2.20078422], parameter b0:[0.4869687] and b1:[0.15808603]\n",
      "5 loss is [2.19657555], parameter b0:[0.4970558] and b1:[0.15968066]\n",
      "6 loss is [2.1940901], parameter b0:[0.50318968] and b1:[0.16066012]\n",
      "7 loss is [2.19260573], parameter b0:[0.50692015] and b1:[0.16125946]\n",
      "8 loss is [2.19171294], parameter b0:[0.50918911] and b1:[0.16162535]\n",
      "9 loss is [2.19117361], parameter b0:[0.51056922] and b1:[0.16184841]\n",
      "10 loss is [2.19084693], parameter b0:[0.5114087] and b1:[0.16198428]\n",
      "11 loss is [2.19064871], parameter b0:[0.51191934] and b1:[0.162067]\n",
      "12 loss is [2.19052833], parameter b0:[0.51222996] and b1:[0.16211734]\n",
      "13 loss is [2.19045517], parameter b0:[0.51241891] and b1:[0.16214797]\n",
      "14 loss is [2.1904107], parameter b0:[0.51253385] and b1:[0.16216661]\n",
      "15 loss is [2.19038365], parameter b0:[0.51260376] and b1:[0.16217795]\n",
      "16 loss is [2.1903672], parameter b0:[0.51264629] and b1:[0.16218484]\n",
      "17 loss is [2.1903572], parameter b0:[0.51267216] and b1:[0.16218904]\n",
      "18 loss is [2.19035111], parameter b0:[0.5126879] and b1:[0.16219159]\n",
      "19 loss is [2.19034741], parameter b0:[0.51269748] and b1:[0.16219314]\n",
      "20 loss is [2.19034516], parameter b0:[0.5127033] and b1:[0.16219409]\n",
      "21 loss is [2.19034379], parameter b0:[0.51270684] and b1:[0.16219466]\n",
      "22 loss is [2.19034296], parameter b0:[0.512709] and b1:[0.16219501]\n",
      "23 loss is [2.19034245], parameter b0:[0.51271031] and b1:[0.16219523]\n",
      "24 loss is [2.19034214], parameter b0:[0.5127111] and b1:[0.16219535]\n",
      "25 loss is [2.19034195], parameter b0:[0.51271159] and b1:[0.16219543]\n",
      "26 loss is [2.19034184], parameter b0:[0.51271188] and b1:[0.16219548]\n",
      "27 loss is [2.19034177], parameter b0:[0.51271206] and b1:[0.16219551]\n",
      "28 loss is [2.19034173], parameter b0:[0.51271217] and b1:[0.16219553]\n",
      "29 loss is [2.1903417], parameter b0:[0.51271224] and b1:[0.16219554]\n",
      "30 loss is [2.19034169], parameter b0:[0.51271228] and b1:[0.16219555]\n",
      "31 loss is [2.19034168], parameter b0:[0.5127123] and b1:[0.16219555]\n",
      "32 loss is [2.19034167], parameter b0:[0.51271232] and b1:[0.16219555]\n",
      "33 loss is [2.19034167], parameter b0:[0.51271233] and b1:[0.16219555]\n",
      "34 loss is [2.19034167], parameter b0:[0.51271233] and b1:[0.16219555]\n",
      "35 loss is [2.19034167], parameter b0:[0.51271234] and b1:[0.16219555]\n",
      "36 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "37 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "38 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "39 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "40 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "41 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "42 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "43 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "44 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "45 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "46 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "47 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "48 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "49 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "50 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "51 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "52 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "53 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "54 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "55 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "56 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "57 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "58 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "59 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "60 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "61 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "62 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "63 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "64 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "65 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "66 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "67 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "68 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "69 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "70 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "71 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "72 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "73 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "74 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "75 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "76 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "77 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "78 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "79 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "80 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "81 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "82 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "83 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "84 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "85 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "86 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "87 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "88 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "89 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "90 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "91 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "92 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "93 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "94 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "95 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "96 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "97 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "98 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "99 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "100 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "101 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "102 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "103 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "104 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "105 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "106 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "107 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "108 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "109 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "110 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "111 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "112 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "113 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "114 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "115 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "116 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "117 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "118 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "119 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "120 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "121 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "122 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "123 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "124 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "125 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "126 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "127 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "128 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "129 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "130 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "131 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "132 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "133 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "134 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "135 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "136 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "137 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "138 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "139 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "140 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "141 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "142 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "143 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "144 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "145 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "146 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "147 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "148 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "149 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "150 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "151 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "152 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "153 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "154 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "155 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "156 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "157 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "158 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "159 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "160 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "161 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "162 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "163 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "164 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "165 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "166 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "167 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "168 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "169 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "170 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "171 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "172 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "173 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "174 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "175 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "176 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "177 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "178 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "179 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "180 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "181 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "182 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "183 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "184 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "185 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "186 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "187 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "188 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "189 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "190 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "191 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "192 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "193 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "194 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "195 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "196 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "197 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "198 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n",
      "199 loss is [2.19034166], parameter b0:[0.51271234] and b1:[0.16219556]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    b0, b1=gradient(x, y, b0, b1, learning_rate)\n",
    "    yhat=b0+b1*x\n",
    "    loss=np.divide(np.sum((y-yhat)**2, axis=0), x.shape[0])\n",
    "    print(f'{epoch} loss is {loss}, parameter b0:{b0} and b1:{b1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e51d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
